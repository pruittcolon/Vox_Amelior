# Gemma Service Dependencies
# FastAPI & Server
fastapi==0.128.0
uvicorn[standard]==0.30.6
pydantic==2.10.6
python-dotenv==1.0.1

# HTTP Client
httpx==0.27.0

# NOTE: llama-cpp-python is installed via pre-built wheel in Dockerfile
# See: docker/wheels/llama_cpp_python-*.whl
# This is built with CUDA support on the host to avoid Docker build issues
