"""
Gemma AI Service - GPU Coordinator Integration
Handles LLM inference with dynamic GPU access via coordinator
"""
import os
import uuid
import asyncio
from typing import Dict, Any, List, Optional
from pathlib import Path
from contextlib import asynccontextmanager
import logging
import inspect

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from llama_cpp import Llama
import httpx
import sys

from gemma_analyzer import GemmaAnalyzer


# Add shared modules to path
sys.path.insert(0, '/app')

# Configure logging (stdout)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger("gemma")

# Configuration
GEMMA_MODEL_PATH = os.getenv("GEMMA_MODEL_PATH", "/app/models/gemma-3-4b-it-UD-Q4_K_XL.gguf")
GEMMA_GPU_LAYERS = int(os.getenv("GEMMA_GPU_LAYERS", "25"))  # Increased for 64k context
GEMMA_CONTEXT_SIZE = int(os.getenv("GEMMA_CONTEXT_SIZE", "8192"))  # Default 8k context; override via env
GEMMA_BATCH_SIZE = int(os.getenv("GEMMA_BATCH_SIZE", "512"))
JWT_ONLY = os.getenv("JWT_ONLY", "false").lower() in {"1", "true", "yes"}
GPU_COORDINATOR_URL = os.getenv("GPU_COORDINATOR_URL", "http://gpu-coordinator:8002")
RAG_SERVICE_URL = os.getenv("RAG_SERVICE_URL", "http://rag-service:8004")
DEFAULT_REPEAT_PENALTY = float(os.getenv("GEMMA_REPEAT_PENALTY", "1.2"))
DEFAULT_REPEAT_LAST_N = int(os.getenv("GEMMA_REPEAT_LAST_N", "256"))
DEFAULT_PENALIZE_NL = os.getenv("GEMMA_PENALIZE_NL", "true").lower() in {"1", "true", "yes"}
DEFAULT_MIROSTAT = int(os.getenv("GEMMA_MIROSTAT", "2"))
DEFAULT_MIROSTAT_TAU = float(os.getenv("GEMMA_MIROSTAT_TAU", "5.0"))
DEFAULT_MIROSTAT_ETA = float(os.getenv("GEMMA_MIROSTAT_ETA", "0.1"))
DEFAULT_STOP_SEQUENCES = [
    token.strip()
    for token in os.getenv("GEMMA_DEFAULT_STOP", "").split(",")
    if token.strip()
]

try:
    _LLAMA_CALL_KWARGS = set(inspect.signature(Llama.__call__).parameters.keys())
except Exception:
    _LLAMA_CALL_KWARGS = set()


def _llama_supports_kwarg(name: str) -> bool:
    return not _LLAMA_CALL_KWARGS or name in _LLAMA_CALL_KWARGS

# Global model instance
gemma_model: Optional[Llama] = None
model_on_gpu: bool = False
service_auth = None
gemma_analyzer: Optional[GemmaAnalyzer] = None


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def merge_stop_sequences(requested: Optional[List[str]]) -> List[str]:
    stops: List[str] = list(DEFAULT_STOP_SEQUENCES)
    if requested:
        for token in requested:
            token = (token or "").strip()
            if token and token not in stops:
                stops.append(token)
    return stops


def decoding_kwargs_from_request(
    *,
    repeat_penalty: Optional[float] = None,
    repeat_last_n: Optional[int] = None,
    penalize_nl: Optional[bool] = None,
    mirostat: Optional[int] = None,
    mirostat_tau: Optional[float] = None,
    mirostat_eta: Optional[float] = None,
) -> Dict[str, Any]:
    kwargs: Dict[str, Any] = {}

    rp = DEFAULT_REPEAT_PENALTY if repeat_penalty is None else repeat_penalty
    if rp is not None and _llama_supports_kwarg("repeat_penalty"):
        kwargs["repeat_penalty"] = rp

    rl = DEFAULT_REPEAT_LAST_N if repeat_last_n is None else repeat_last_n
    if rl and rl > 0 and _llama_supports_kwarg("repeat_last_n"):
        kwargs["repeat_last_n"] = rl

    penalize_newlines = DEFAULT_PENALIZE_NL if penalize_nl is None else penalize_nl
    if _llama_supports_kwarg("penalize_nl"):
        kwargs["penalize_nl"] = penalize_newlines

    mirostat_mode = DEFAULT_MIROSTAT if mirostat is None else mirostat
    if mirostat_mode and _llama_supports_kwarg("mirostat"):
        kwargs["mirostat"] = mirostat_mode
        if _llama_supports_kwarg("mirostat_tau"):
            kwargs["mirostat_tau"] = DEFAULT_MIROSTAT_TAU if mirostat_tau is None else mirostat_tau
        if _llama_supports_kwarg("mirostat_eta"):
            kwargs["mirostat_eta"] = DEFAULT_MIROSTAT_ETA if mirostat_eta is None else mirostat_eta

    return kwargs


def get_service_headers(expires_in: int = 60) -> Dict[str, str]:
    """Get service authentication headers (JWT only)."""
    if not service_auth:
        raise RuntimeError("Service authentication not initialized")
    try:
        token = service_auth.create_token(expires_in=expires_in)
        return {"X-Service-Token": token}
    except Exception as exc:
        logger.error(f"[GEMMA] Failed to create service JWT: {exc}")
        raise


# ============================================================================
# MODEL MANAGEMENT
# ============================================================================

def load_model_cpu():
    """Load model on CPU/RAM - ready for GPU transfer"""
    global gemma_model, model_on_gpu
    
    logger.info(f"[GEMMA] Loading model from: {GEMMA_MODEL_PATH}")
    logger.info(f"[GEMMA] Mode: CPU (n_gpu_layers=0)")
    logger.info(f"[GEMMA] Context size: {GEMMA_CONTEXT_SIZE} tokens")
    
    if not Path(GEMMA_MODEL_PATH).exists():
        raise RuntimeError(f"Model file not found: {GEMMA_MODEL_PATH}")
    
    try:
        gemma_model = Llama(
            model_path=GEMMA_MODEL_PATH,
            n_ctx=GEMMA_CONTEXT_SIZE,
            n_batch=128,  # Smaller batch for stability
            n_gpu_layers=0,  # CPU mode
            n_threads=6,
            use_mlock=False,  # Don't lock memory
            use_mmap=True,  # Use memory mapping
            verbose=False,
        )
        model_on_gpu = False
        logger.info("[GEMMA] Model loaded on CPU/RAM successfully!")
        
    except Exception as e:
        logger.error(f"[GEMMA] ERROR loading model: {e}")
        raise


def load_model_gpu():
    """Load model directly on GPU at startup"""
    global gemma_model, model_on_gpu
    
    logger.info(f"[GEMMA] Loading model from: {GEMMA_MODEL_PATH}")
    logger.info(f"[GEMMA] Mode: GPU (n_gpu_layers={GEMMA_GPU_LAYERS})")
    logger.info(f"[GEMMA] Context size: {GEMMA_CONTEXT_SIZE} tokens")
    
    if not Path(GEMMA_MODEL_PATH).exists():
        raise RuntimeError(f"Model file not found: {GEMMA_MODEL_PATH}")
    
    gemma_model = Llama(
        model_path=GEMMA_MODEL_PATH,
        n_ctx=GEMMA_CONTEXT_SIZE,  # Context window
        n_batch=512,  # Larger batch for faster prompt processing
        n_gpu_layers=-1,  # ALL layers on GPU
        n_threads=4,  # Fewer CPU threads since GPU does the work
        use_mlock=False,  # Critical: don't lock memory
        use_mmap=True,  # Use memory mapping for efficiency
        flash_attn=True,  # Re-enable flash attention for performance
        verbose=True,  # Enable verbose for debugging
    )
    model_on_gpu = True
    logger.info("[GEMMA] Model loaded on GPU successfully!")
    
    # Log VRAM usage
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used", "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True,
            timeout=2
        )
        if result.returncode == 0:
            vram_used = result.stdout.strip().split('\n')[0]
            logger.info(f"[GEMMA] VRAM used: {vram_used} MB")
    except Exception as e:
        logger.warning(f"[GEMMA] Could not query VRAM: {e}")


def move_model_to_gpu():
    """Move model from CPU to GPU for fast inference"""
    global gemma_model, model_on_gpu
    
    if model_on_gpu:
        logger.info("[GEMMA] Model already on GPU")
        return
    
    logger.info("[GEMMA] üöÄ Moving model from CPU to GPU for fast inference...")
    
    # Save reference to old model in case we need to restore
    old_model = gemma_model
    
    try:
        # Delete CPU model to free RAM
        del gemma_model
        gemma_model = None
        
        # Reload on GPU
        load_model_gpu()
        
        logger.info("[GEMMA] ‚úÖ Model moved to GPU successfully")
        
    except Exception as e:
        logger.error(f"[GEMMA] ‚ùå ERROR moving to GPU: {e}")
        logger.info("[GEMMA] Keeping model on CPU...")
        # Restore old model or reload on CPU
        if old_model is not None:
            gemma_model = old_model
            model_on_gpu = False
        else:
            load_model_cpu()
        raise


def move_model_to_cpu():
    """Move model from GPU back to CPU to free VRAM"""
    global gemma_model, model_on_gpu
    
    if not model_on_gpu:
        logger.info("[GEMMA] Model already on CPU")
        return
    
    logger.info("[GEMMA] üíæ Moving model from GPU back to CPU to free VRAM...")
    
    try:
        # Delete GPU model to free VRAM
        del gemma_model
        
        # Reload on CPU (background mode)
        load_model_cpu()
        
        logger.info("[GEMMA] ‚úÖ Model moved back to CPU successfully")
        
    except Exception as e:
        logger.error(f"[GEMMA] ‚ùå ERROR moving to CPU: {e}")
        raise


class ServiceAuthMiddleware(BaseHTTPMiddleware):
    """Middleware for JWT service authentication (Phase 3: Enforce JWT-only + replay)."""
    async def dispatch(self, request: Request, call_next):
        # Skip auth for health checks (internal utility)
        if request.url.path in ["/health"]:
            return await call_next(request)

        jwt_token = request.headers.get("X-Service-Token")
        if not jwt_token or not service_auth:
            logger.error(f"‚ùå Missing JWT for {request.url.path}")
            return JSONResponse(status_code=401, content={"detail": "Missing service token"})

        try:
            # Allowed callers: gateway, transcription-service (signals)
            allowed = ["gateway", "transcription-service"]
            payload = service_auth.verify_token(jwt_token, allowed_services=allowed, expected_aud="internal")

            # Replay protection
            from shared.security.service_auth import get_replay_protector
            import time as _t
            ttl = max(10, int(payload["expires_at"] - _t.time()) + 10)
            ok, reason = get_replay_protector().check_and_store(payload.get("request_id", ""), ttl)
            if not ok:
                logger.error(f"‚ùå JWT replay blocked: reason={reason}")
                return JSONResponse(status_code=401, content={"detail": "Replay detected"})

            rid_short = str(payload.get('request_id',''))[:8]
            logger.info(f"‚úÖ JWT OK s={payload.get('service_id')} aud=internal rid={rid_short} path={request.url.path}")
            return await call_next(request)
        except Exception as e:
            logger.error(f"‚ùå JWT rejected: {e} path={request.url.path}")
            return JSONResponse(status_code=401, content={"detail": "Invalid service token"})


class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """Adds request_id, analysis_id, and structured access logs with latency."""
    async def dispatch(self, request: Request, call_next):
        import time
        req_id = request.headers.get("X-Request-Id") or uuid.uuid4().hex[:12]
        analysis_id = request.headers.get("X-Analysis-Id") or request.query_params.get("analysis_id")
        start = time.monotonic()
        path = request.url.path
        method = request.method
        try:
            response = await call_next(request)
            duration_ms = int((time.monotonic() - start) * 1000)
            try:
                response.headers["X-Request-Id"] = req_id
            except Exception:
                pass
            logger.info(
                f"[ACCESS] {method} {path} {response.status_code} rid={req_id} analysis_id={analysis_id or '-'} {duration_ms}ms"
            )
            return response
        except Exception as exc:  # noqa: BLE001
            duration_ms = int((time.monotonic() - start) * 1000)
            logger.error(
                f"[ERROR] {method} {path} rid={req_id} analysis_id={analysis_id or '-'} {duration_ms}ms err={exc}"
            )
            raise


# Lifespan context manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage startup and shutdown"""
    # Startup
    logger.info("Starting Gemma AI Service...")
    logger.info("=" * 80)
    
    # Initialize service auth (Phase 3)
    global service_auth, gemma_analyzer
    try:
        from shared.security.service_auth import get_service_auth, load_service_jwt_keys
        jwt_keys = load_service_jwt_keys("gemma-service")
        service_auth = get_service_auth(service_id="gemma-service", service_secret=jwt_keys)
        logger.info(
            "‚úÖ JWT service auth initialized (enforcing JWT-only, aud=internal, replay protected, keys=%s)",
            len(jwt_keys),
        )
    except Exception as e:
        logger.error(f"‚ùå JWT service auth initialization failed: {e}")
        raise
    
    # START ON GPU DIRECTLY (transcription now starts on CPU)
    # With START_ON_CPU=true for transcription, Gemma can claim GPU at startup
    logger.info("[GEMMA] üöÄ Loading model DIRECTLY on GPU (transcription on CPU)...")
    try:
        load_model_gpu()
        gemma_analyzer = GemmaAnalyzer(rag_url=RAG_SERVICE_URL)
        logger.info("[GEMMA] ‚úÖ Loaded on GPU successfully with full 25k context!")
        logger.info("Gemma AI Service started successfully (GPU mode)")
    except Exception as e:
        logger.error(f"[GEMMA] ‚ùå Failed to load on GPU: {e}")
        logger.info("[GEMMA] ÔøΩ Falling back to CPU mode...")
        load_model_cpu()
        gemma_analyzer = GemmaAnalyzer(rag_url=RAG_SERVICE_URL)
        logger.info("[GEMMA] ‚úÖ Loaded on CPU successfully!")
        logger.info("Gemma AI Service started (CPU fallback mode)")
    logger.info("=" * 80)

    yield
    
    # Shutdown
    logger.info("Shutting down Gemma AI Service...")
    global gemma_model
    if gemma_model:
        del gemma_model
    logger.info("Gemma AI Service shutdown complete")


# Initialize FastAPI
app = FastAPI(
    title="Gemma AI Service",
    description="LLM inference service with dynamic GPU access",
    version="1.0.0",
    lifespan=lifespan
)

# Add JWT middleware (Phase 2: Permissive)
app.add_middleware(ServiceAuthMiddleware)
app.add_middleware(RequestLoggingMiddleware)

# CORS (Allow all for local playground)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# REQUEST MODELS
# ============================================================================

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    stop: Optional[List[str]] = None
    repeat_penalty: Optional[float] = None
    repeat_last_n: Optional[int] = None
    penalize_nl: Optional[bool] = None
    mirostat: Optional[int] = None
    mirostat_tau: Optional[float] = None
    mirostat_eta: Optional[float] = None

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    context: Optional[List[Dict[str, Any]]] = None
    stop: Optional[List[str]] = None
    repeat_penalty: Optional[float] = None
    repeat_last_n: Optional[int] = None
    penalize_nl: Optional[bool] = None
    mirostat: Optional[int] = None
    mirostat_tau: Optional[float] = None
    mirostat_eta: Optional[float] = None


class GemmaAnalyzeRequest(BaseModel):
    filters: Dict[str, Any] = Field(default_factory=dict)
    custom_prompt: Optional[str] = None
    max_tokens: int = 1024
    temperature: float = 0.3




class RAGChatRequest(BaseModel):
    """RAG-enhanced chat request"""
    query: str
    max_tokens: int = 500
    temperature: float = 0.7
    top_p: float = 0.9
    session_id: Optional[str] = None
    top_k_results: int = 8
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    last_n_transcripts: Optional[int] = None
    speakers: Optional[List[str]] = None
    bias_emotions: Optional[List[str]] = None
    use_memories: bool = False
    include_memories_top_k: int = 3
    stop: Optional[List[str]] = None
    repeat_penalty: Optional[float] = None
    repeat_last_n: Optional[int] = None
    penalize_nl: Optional[bool] = None
    mirostat: Optional[int] = None
    mirostat_tau: Optional[float] = None
    mirostat_eta: Optional[float] = None

# ============================================================================
# ROUTES
# ============================================================================

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    status = {
        "status": "healthy" if gemma_model is not None else "unhealthy",
        "model_loaded": gemma_model is not None,
        "model_on_gpu": model_on_gpu,
        "model_path": GEMMA_MODEL_PATH,
        "gpu_layers": GEMMA_GPU_LAYERS,
        "context_size": GEMMA_CONTEXT_SIZE,
    }
    
    # Try to get VRAM usage
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used,memory.total", "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            vram_info = result.stdout.strip().split('\n')[0].split(',')
            status["vram_used_mb"] = int(vram_info[0].strip())
            status["vram_total_mb"] = int(vram_info[1].strip())
    except Exception:
        pass
    
    return status


@app.post("/move-to-cpu")
async def move_to_cpu_endpoint():
    """Move model from GPU to CPU (called when other services start)"""
    global model_on_gpu
    
    if not model_on_gpu:
        return {"status": "already_on_cpu", "message": "Model is already on CPU"}
    
    logger.info("[GEMMA] üì° Received signal to move to CPU (other services starting)")
    
    try:
        move_model_to_cpu()
        return {
            "status": "success",
            "message": "Model moved to CPU, GPU freed for transcription",
            "model_on_gpu": False
        }
    except Exception as e:
        logger.error(f"[GEMMA] Failed to move to CPU: {e}")
        return {"status": "error", "message": str(e)}


@app.post("/warmup")
async def warmup_gpu():
    """
    Warmup endpoint - moves model to GPU and waits until ready
    Call this BEFORE sending your actual generate request
    """
    global model_on_gpu, gemma_model
    
    if gemma_model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    logger.info("[GEMMA] üî• Warmup requested - preparing GPU...")
    task_id = f"gemma-warmup-{uuid.uuid4().hex[:8]}"
    gpu_acquired = False
    response_payload: Optional[Dict[str, Any]] = None
    release_payload: Optional[Dict[str, Any]] = None
    
    try:
        logger.info(f"üì° [WARMUP] Requesting GPU slot from coordinator (task={task_id})...")
        async with httpx.AsyncClient(timeout=30.0) as client:
            coord_response = await client.post(
                f"{GPU_COORDINATOR_URL}/gemma/request",
                json={
                    "task_id": task_id,
                    "messages": [{"role": "system", "content": "warmup"}],
                    "max_tokens": 1,
                    "temperature": 0.7
                },
                headers=get_service_headers()
            )
            if coord_response.status_code != 200:
                logger.error(f"‚ùå [WARMUP] Failed to acquire GPU slot (status {coord_response.status_code})")
                raise HTTPException(status_code=503, detail="Failed to acquire GPU slot")
        gpu_acquired = True
        
        if not model_on_gpu:
            logger.info("[GEMMA] üöÄ Attempting to move model to GPU for warmup...")
            try:
                move_model_to_gpu()
                logger.info("[GEMMA] ‚úÖ Model ready on GPU!")
                response_payload = {
                    "status": "ready",
                    "message": "Model warmed up and ready on GPU",
                    "model_on_gpu": True,
                    "task_id": task_id
                }
                release_payload = {"result": {"status": "warmup", "model_on_gpu": True}}
            except Exception as gpu_error:
                logger.warning(f"[GEMMA] ‚ö†Ô∏è Could not move to GPU: {gpu_error}")
                logger.info("[GEMMA] üíæ GPU load failed - continuing on CPU")
                response_payload = {
                    "status": "ready_cpu",
                    "message": "Model ready on CPU (GPU unavailable)",
                    "model_on_gpu": False,
                    "task_id": task_id,
                    "warning": "Using CPU - inference will be slower"
                }
                release_payload = {"result": {"status": "warmup_cpu", "warning": str(gpu_error)}}
        else:
            logger.info("[GEMMA] ‚úÖ Model already on GPU!")
            response_payload = {
                "status": "ready",
                "message": "Model already on GPU",
                "model_on_gpu": True,
                "task_id": task_id
            }
            release_payload = {"result": {"status": "warmup_cached", "model_on_gpu": True}}
    except HTTPException:
        release_payload = {"result": {"error": "warmup_failed_before_acquire"}}
        raise
    except Exception as e:
        release_payload = {"result": {"error": str(e)}}
        raise
    finally:
        if gpu_acquired:
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    await client.post(
                        f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                        json=release_payload or {"result": {"status": "warmup_unknown"}},
                        headers=get_service_headers()
                    )
                logger.info("[GEMMA] ‚úÖ Released GPU slot, transcription can resume")
            except Exception as release_error:
                logger.error(f"[GEMMA] ‚ùå Failed to release GPU slot: {release_error}")
    
    return response_payload


async def run_llm_task(
    task_prefix: str,
    prompt: str,
    max_tokens: int,
    temperature: float,
    *,
    top_p: float = 0.9,
    stop: Optional[List[str]] = None,
    decoding_overrides: Optional[Dict[str, Any]] = None,
):
    if gemma_model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    task_id = f"{task_prefix}-{uuid.uuid4().hex[:12]}"
    logger.info("=" * 80)
    logger.info(f"üîÑ [TASK {task_id}] Prompt preview: {prompt[:100]}...")
    logger.info(f"üîÑ [TASK {task_id}] Max tokens: {max_tokens}, Temp: {temperature}")

    gpu_slot_acquired = False
    using_cpu_fallback = False
    try:
        logger.info(f"üì° [TASK {task_id}] Requesting GPU slot from coordinator...")
        async with httpx.AsyncClient(timeout=30.0) as client:
            coord_response = await client.post(
                f"{GPU_COORDINATOR_URL}/gemma/request",
                json={
                    "task_id": task_id,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                },
                headers=get_service_headers(),
            )
            if coord_response.status_code != 200:
                logger.error(f"‚ùå [TASK {task_id}] Failed to acquire GPU slot: {coord_response.status_code}")
                raise HTTPException(status_code=503, detail="Failed to acquire GPU slot from coordinator")
        gpu_slot_acquired = True

        if not model_on_gpu:
            logger.warning(f"‚ö†Ô∏è [TASK {task_id}] Model not on GPU! Attempting warmup...")
            try:
                move_model_to_gpu()
                logger.info(f"üöÄ [TASK {task_id}] Moved to GPU, executing inference...")
            except Exception as gpu_error:
                logger.warning(f"‚ö†Ô∏è [TASK {task_id}] GPU load failed, using CPU: {gpu_error}")
                using_cpu_fallback = True
                try:
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        await client.post(
                            f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                            json={"error": "GPU unavailable - using CPU"},
                            headers=get_service_headers(),
                        )
                    gpu_slot_acquired = False
                except Exception as release_error:
                    logger.error(f"‚ùå Failed to release GPU slot: {release_error}")
        else:
            logger.info(f"üöÄ [TASK {task_id}] Executing inference on GPU (already warmed up)...")

        stop_sequences = merge_stop_sequences(stop)
        llama_kwargs: Dict[str, Any] = {
            "prompt": prompt,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "stop": stop_sequences,
            "echo": False,
        }
        if decoding_overrides:
            llama_kwargs.update(
                decoding_kwargs_from_request(
                    repeat_penalty=decoding_overrides.get("repeat_penalty"),
                    repeat_last_n=decoding_overrides.get("repeat_last_n"),
                    penalize_nl=decoding_overrides.get("penalize_nl"),
                    mirostat=decoding_overrides.get("mirostat"),
                    mirostat_tau=decoding_overrides.get("mirostat_tau"),
                    mirostat_eta=decoding_overrides.get("mirostat_eta"),
                )
            )
        else:
            llama_kwargs.update(decoding_kwargs_from_request())

        # CRITICAL: Reset model state before each inference to prevent KV cache buildup
        try:
            gemma_model.reset()
            logger.debug(f"[TASK {task_id}] Model state reset before generation")
        except Exception as e:
            logger.warning(f"[TASK {task_id}] Could not reset model state: {e}")

        response = gemma_model(**llama_kwargs)
        usage = response.get("usage", {})
        model_name = response.get("model", "gemma-3-4b-it")
        result = {
            "text": response["choices"][0]["text"],
            "tokens_generated": usage.get("completion_tokens"),
            "task_id": task_id,
            "mode": "cpu" if using_cpu_fallback else "gpu",
            "usage": usage,
            "model": model_name,
        }

        try:
            move_model_to_cpu()
            logger.info(f"üíæ [TASK {task_id}] Model back to CPU, VRAM freed")
        except Exception as cpu_error:
            logger.warning(f"‚ö†Ô∏è [TASK {task_id}] Failed to move back to CPU: {cpu_error}")

        if not using_cpu_fallback and gpu_slot_acquired:
            logger.info(f"üîì [TASK {task_id}] Releasing GPU slot...")
            async with httpx.AsyncClient(timeout=10.0) as client:
                await client.post(
                    f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                    json={"result": result},
                    headers=get_service_headers(),
                )
            gpu_slot_acquired = False
        else:
            logger.info(f"‚úÖ [TASK {task_id}] Complete (GPU slot already released or CPU fallback)")

        logger.info(f"‚úÖ [TASK {task_id}] Inference complete - Generated {result.get('tokens_generated')} tokens")
        logger.info(f"üìù [TASK {task_id}] Response preview: {result['text'][:200]}...")
        logger.info("=" * 80)
        return result

    except Exception as e:
        logger.error(f"‚ùå [TASK {task_id}] Error: {e}")
        logger.error("=" * 80)
        try:
            if gpu_slot_acquired:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    await client.post(
                        f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                        json={"result": {"error": str(e)}},
                        headers=get_service_headers(),
                    )
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/generate")
async def generate_text(request: GenerateRequest):
    return await run_llm_task(
        "gemma-gen",
        request.prompt,
        request.max_tokens,
        request.temperature,
        top_p=request.top_p,
        stop=request.stop,
        decoding_overrides={
            "repeat_penalty": request.repeat_penalty,
            "repeat_last_n": request.repeat_last_n,
            "penalize_nl": request.penalize_nl,
            "mirostat": request.mirostat,
            "mirostat_tau": request.mirostat_tau,
            "mirostat_eta": request.mirostat_eta,
        },
    )


@app.post("/analyze")
async def gemma_quick_analyze(request: GemmaAnalyzeRequest):
    if gemma_analyzer is None:
        raise HTTPException(status_code=503, detail="Analyzer unavailable")

    filters = request.filters or {}
    try:
        service_headers = get_service_headers(expires_in=120)
    except Exception as exc:
        logger.warning(f"[ANALYZE] Unable to create service headers: {exc}")
        service_headers = None

    async def _llm(prompt: str, max_tokens: int, temperature: float):
        return await run_llm_task(
            "gemma-ana",
            prompt,
            max_tokens,
            temperature,
        )

    try:
        return await gemma_analyzer.run_analysis(
            llm_callable=_llm,
            filters=filters,
            custom_prompt=request.custom_prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            service_headers=service_headers,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error(f"[ANALYZE] Failed: {exc}")
        raise HTTPException(status_code=500, detail=str(exc))


@app.post("/chat")
async def chat(
    request: ChatRequest
):
    """
    Chat with conversation history
    Uses GPU coordinator for exclusive GPU access
    """
    if gemma_model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    task_id = f"gemma-chat-{uuid.uuid4().hex[:12]}"
    
    logger.info("=" * 80)
    logger.info(f"üí¨ [CHAT] Task {task_id} started")
    logger.info(f"üí¨ [CHAT] Messages: {len(request.messages)}, Max tokens: {request.max_tokens}")
    
    try:
        # Request GPU from coordinator
        logger.info(f"üì° [TASK {task_id}] Requesting GPU slot from coordinator...")
        async with httpx.AsyncClient(timeout=30.0) as client:
            coord_response = await client.post(
                f"{GPU_COORDINATOR_URL}/gemma/request",
                json={
                    "task_id": task_id,
                    "messages": [{"role": m.role, "content": m.content} for m in request.messages],
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature,
                    "context": request.context or []
                },
                headers=get_service_headers()
            )
            
            if coord_response.status_code != 200:
                logger.error(f"‚ùå [TASK {task_id}] Failed to acquire GPU slot: {coord_response.status_code}")
                raise HTTPException(status_code=503, detail="Failed to acquire GPU slot from coordinator")
            
            logger.info(f"‚úÖ [TASK {task_id}] GPU slot acquired")
        gpu_slot_acquired = True
        gpu_slot_released = False
        
        # Move model to GPU if it's on CPU
        if not model_on_gpu:
            logger.info(f"[TASK {task_id}] Moving model to GPU for inference...")
            move_model_to_gpu()
        
        logger.info(f"[TASK {task_id}] Using model in {('GPU' if model_on_gpu else 'CPU')} mode")

        # Build prompt from messages
        prompt = "You are a helpful AI assistant. Always respond in English.\n\n"
        
        # Add context if provided
        if request.context:
            prompt += "Context from memories:\n"
            for ctx in request.context[:3]:  # Top 3 memories
                prompt += f"- {ctx.get('content', '')[:200]}\n"
            prompt += "\n"
        
        # Add conversation history
        for msg in request.messages:
            role = msg.role.upper()
            prompt += f"{role}: {msg.content}\n"
        
        prompt += "ASSISTANT: "
        
        logger.info(f"[TASK {task_id}] Executing inference...")
        
        # Track inference time
        import time
        start_time = time.time()
        
        # CRITICAL: Reset model state before each inference to prevent KV cache buildup
        # This clears the internal context from previous calls, ensuring each request
        # starts fresh and doesn't accumulate tokens that could exhaust the context window
        try:
            gemma_model.reset()
            logger.debug(f"[TASK {task_id}] Model state reset successfully")
        except Exception as e:
            logger.warning(f"[TASK {task_id}] Could not reset model state: {e}")
        
        stop_sequences = merge_stop_sequences(request.stop)
        llama_kwargs = {
            "prompt": prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "echo": False,
            "stop": stop_sequences,
        }
        llama_kwargs.update(
            decoding_kwargs_from_request(
                repeat_penalty=request.repeat_penalty,
                repeat_last_n=request.repeat_last_n,
                penalize_nl=request.penalize_nl,
                mirostat=request.mirostat,
                mirostat_tau=request.mirostat_tau,
                mirostat_eta=request.mirostat_eta,
            )
        )

        response = gemma_model(**llama_kwargs)
        
        generated_text = response["choices"][0]["text"].strip()
        tokens_generated = response["usage"].get("completion_tokens", 0)
        prompt_tokens = response["usage"].get("prompt_tokens", 0)
        
        # Calculate inference time and tokens/sec
        import time
        inference_end = time.time()
        inference_time = inference_end - start_time if 'start_time' in locals() else 0
        tokens_per_sec = tokens_generated / inference_time if inference_time > 0 else 0

        if tokens_generated == 0:
            logger.warning(f"‚ö†Ô∏è [TASK {task_id}] Chat generated 0 tokens. Check stop tokens/prompt.")
        
        # Get VRAM usage stats
        vram_stats = {}
        try:
            import subprocess
            vram_result = subprocess.run(
                ["nvidia-smi", "--query-gpu=memory.used,memory.total,utilization.gpu", 
                 "--format=csv,noheader,nounits"],
                capture_output=True,
                text=True,
                timeout=2
            )
            if vram_result.returncode == 0:
                info = vram_result.stdout.strip().split('\n')[0].split(',')
                vram_stats = {
                    "vram_used_mb": int(info[0].strip()),
                    "vram_total_mb": int(info[1].strip()),
                    "gpu_utilization": int(info[2].strip())
                }
        except Exception as e:
            logger.debug(f"Could not query VRAM: {e}")

        result = {
            "message": generated_text or "",
            "tokens_generated": tokens_generated,
            "prompt_tokens": prompt_tokens,
            "total_tokens": tokens_generated + prompt_tokens,
            "task_id": task_id,
            "mode": "gpu" if model_on_gpu else "cpu",
            "inference_time_sec": round(inference_time, 2),
            "tokens_per_sec": round(tokens_per_sec, 1),
            **vram_stats
        }
        
        logger.info(f"‚úÖ [TASK {task_id}] Inference complete - Generated {result['tokens_generated']} tokens")
        logger.info(f"üí¨ [TASK {task_id}] Response: {result['message'][:200]}...")
        
        # Release GPU slot back to coordinator
        if gpu_slot_acquired and not gpu_slot_released:
            logger.info(f"üîì [TASK {task_id}] Releasing GPU slot...")
            async with httpx.AsyncClient(timeout=10.0) as client:
                await client.post(
                    f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                    json={"result": result},
                headers=get_service_headers()
            )
            logger.info(f"‚úÖ [TASK {task_id}] GPU slot released, model remains on GPU")
            gpu_slot_released = True
        logger.info("=" * 80)
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå [TASK {task_id}] Error: {e}")
        logger.error("=" * 80)
        
        # Release GPU on error if still held
        try:
            if gpu_slot_acquired and not gpu_slot_released:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    await client.post(
                        f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                        json={"result": {"error": str(e)}},
                        headers=get_service_headers()
                    )
        except Exception:
            pass
        
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/rag")
async def chat_with_rag(
    request: RAGChatRequest
):
    """
    RAG-enhanced chat: Retrieves relevant context from transcripts, then generates answer
    
    Flow:
    1. Search RAG service for relevant segments
    2. Build context prompt with results
    3. Generate answer with Gemma
    4. Return answer with sources
    """
    if gemma_model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    task_id = f"gemma-rag-{uuid.uuid4().hex[:12]}"
    gpu_slot_acquired = False
    gpu_slot_released = False
    used_gpu = False

    def format_timestamp(seconds: Optional[float]) -> Optional[str]:
        try:
            value = float(seconds)
        except (TypeError, ValueError):
            return None
        minutes = int(value // 60)
        secs = int(value % 60)
        return f"{minutes:02d}:{secs:02d}"

    async def fetch_rag_results(doc_type: str, top_k: int) -> List[Dict[str, Any]]:
        payload = {
            "query": request.query,
            "top_k": top_k,
            "doc_type": doc_type,
            "start_date": request.start_date,
            "end_date": request.end_date,
            "last_n_transcripts": request.last_n_transcripts,
            "speakers": request.speakers,
            "bias_emotions": request.bias_emotions,
        }
        async with httpx.AsyncClient(timeout=15.0) as client:
            response = await client.post(
                f"{RAG_SERVICE_URL}/search/semantic",
                json=payload,
                headers=get_service_headers(),
            )
        if response.status_code != 200:
            logger.warning(f"[TASK {task_id}] RAG search {doc_type} failed: {response.status_code}")
            return []
        data = response.json()
        results = data.get("results", [])
        logger.info(f"[TASK {task_id}] Retrieved {len(results)} {doc_type} results")
        return results

    try:
        transcript_results = await fetch_rag_results("transcript_segment", request.top_k_results)
        memory_results: List[Dict[str, Any]] = []
        if request.use_memories:
            memory_results = await fetch_rag_results("memory", request.include_memories_top_k)

        if not transcript_results and not memory_results:
            logger.info(f"[TASK {task_id}] No RAG context found for query")
            return {
                "answer": "I couldn't find any conversation excerpts that answer that question within the selected scope.",
                "mode": "cpu" if not model_on_gpu else "gpu",
                "tokens_generated": 0,
                "confidence": 0.0,
                "sources": [],
                "task_id": task_id,
            }

        context_lines: List[str] = []
        sources: List[Dict[str, Any]] = []
        citation_index = 1

        def append_context(result_list: List[Dict[str, Any]], prefix: str = "C") -> None:
            nonlocal citation_index
            for result in result_list:
                metadata = result.get("metadata") or {}
                text = result.get("text") or ""
                label = f"{prefix}{citation_index}"

                if result.get("type") == "memory":
                    title = metadata.get("title") or "Memory"
                    context_lines.append(f"[{label}] Memory ‚Ä¢ {title}: \"{text}\"")
                    sources.append({
                        "label": label,
                        "type": "memory",
                        "title": title,
                        "text_preview": text[:180],
                        "score": result.get("score", 0.0),
                        "created_at": metadata.get("created_at"),
                        "memory_id": metadata.get("memory_id"),
                    })
                else:
                    speaker = metadata.get("speaker") or "Unknown speaker"
                    emotion = metadata.get("emotion") or "neutral"
                    timestamp_display = format_timestamp(metadata.get("start_time")) or "--:--"
                    context_lines.append(f"[{label}] {timestamp_display} ‚Ä¢ {speaker} ({emotion}): \"{text}\"")
                    sources.append({
                        "label": label,
                        "type": "transcript_segment",
                        "speaker": speaker,
                        "timestamp": timestamp_display,
                        "start_time": metadata.get("start_time"),
                        "end_time": metadata.get("end_time"),
                        "job_id": metadata.get("job_id"),
                        "transcript_id": metadata.get("transcript_id"),
                        "segment_id": metadata.get("segment_id"),
                        "emotion": emotion,
                        "score": result.get("score", 0.0),
                        "text_preview": text[:180],
                        "created_at": metadata.get("created_at"),
                    })
                citation_index += 1

        append_context(transcript_results[: request.top_k_results])
        if memory_results:
            append_context(memory_results[: request.include_memories_top_k], prefix="M")

        instruction = (
            "You are an assistant that answers questions using the provided conversation excerpts. "
            "Each excerpt has a label like [C1]. Use those labels in your answer as citations. "
            "If the excerpts do not contain the answer, say that you cannot find evidence."
        )
        context_block = "\n".join(context_lines)
        prompt = (
            f"{instruction}\n\n"
            f"Conversation excerpts:\n{context_block}\n\n"
            f"User question: {request.query}\n\n"
            "Answer:"
        )

        logger.info(f"[TASK {task_id}] Prompt prepared with {len(context_lines)} excerpts")

        # Acquire GPU slot
        logger.info(f"[TASK {task_id}] Requesting GPU access‚Ä¶")
        async with httpx.AsyncClient(timeout=30.0) as client:
            coord_response = await client.post(
                f"{GPU_COORDINATOR_URL}/gemma/request",
                json={
                    "task_id": task_id,
                    "messages": [{"role": "user", "content": request.query}],
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature,
                },
                headers=get_service_headers()
            )
        if coord_response.status_code != 200:
            raise HTTPException(status_code=503, detail="Failed to acquire GPU")
        gpu_slot_acquired = True

        if not model_on_gpu:
            try:
                move_model_to_gpu()
                used_gpu = True
            except Exception as gpu_error:
                logger.warning(f"‚ö†Ô∏è [TASK {task_id}] Failed to move model to GPU: {gpu_error}. Falling back to CPU.")
                async with httpx.AsyncClient(timeout=10.0) as client:
                    await client.post(
                        f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                        json={"result": {"warning": str(gpu_error), "mode": "cpu"}},
                    headers=get_service_headers()
                    )
                gpu_slot_acquired = False
                gpu_slot_released = True
        else:
            used_gpu = True

        stop_sequences = merge_stop_sequences(request.stop)
        generation_kwargs = {
            "prompt": prompt,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "echo": False,
            "stop": stop_sequences,
        }
        generation_kwargs.update(
            decoding_kwargs_from_request(
                repeat_penalty=request.repeat_penalty,
                repeat_last_n=request.repeat_last_n,
                penalize_nl=request.penalize_nl,
                mirostat=request.mirostat,
                mirostat_tau=request.mirostat_tau,
                mirostat_eta=request.mirostat_eta,
            )
        )

        # CRITICAL: Reset model state before each inference to prevent KV cache buildup
        try:
            gemma_model.reset()
            logger.debug(f"[TASK {task_id}] Model state reset before RAG generation")
        except Exception as e:
            logger.warning(f"[TASK {task_id}] Could not reset model state: {e}")

        response = gemma_model(**generation_kwargs)
        answer_text = response["choices"][0]["text"].strip()
        tokens_generated = response["usage"].get("completion_tokens", 0)

        if tokens_generated == 0:
            logger.warning(f"‚ö†Ô∏è [TASK {task_id}] Generated 0 tokens; check prompt/stop settings")

        score_values = [res.get("score", 0.0) for res in transcript_results[:3] if isinstance(res, dict)]
        if score_values:
            avg_score = sum(score_values) / len(score_values)
            confidence = max(0.0, min(0.99, (avg_score + 1.0) / 2.0))
        else:
            confidence = 0.4 if answer_text else 0.0

        result = {
            "answer": answer_text or "I couldn't derive an answer from the provided excerpts.",
            "query": request.query,
            "sources": sources,
            "sources_count": len(sources),
            "tokens_generated": tokens_generated,
            "task_id": task_id,
            "has_context": bool(transcript_results or memory_results),
            "mode": "gpu" if model_on_gpu else "cpu",
            "confidence": confidence,
        }

        if gpu_slot_acquired and not gpu_slot_released:
            async with httpx.AsyncClient(timeout=10.0) as client:
                await client.post(
                    f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                    json={"result": result},
                headers=get_service_headers()
                )
            gpu_slot_released = True

        if model_on_gpu and used_gpu:
            move_model_to_cpu()

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[TASK {task_id}] Error: {e}")
        import traceback
        traceback.print_exc()

        try:
            if gpu_slot_acquired and not gpu_slot_released:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    await client.post(
                        f"{GPU_COORDINATOR_URL}/gemma/release/{task_id}",
                        json={"result": {"error": str(e)}},
                    headers=get_service_headers()
                    )
        except Exception as release_error:
            logger.error(f"[TASK {task_id}] Failed to release GPU after error: {release_error}")

        try:
            if model_on_gpu and used_gpu:
                move_model_to_cpu()
        except Exception as cpu_error:
            logger.error(f"[TASK {task_id}] Failed to move model back to CPU after error: {cpu_error}")

        raise HTTPException(status_code=500, detail=str(e))


@app.get("/stats")
async def get_stats():
    """Get service statistics"""
    stats = {
        "model_loaded": gemma_model is not None,
        "model_on_gpu": model_on_gpu,
        "gpu_layers": GEMMA_GPU_LAYERS,
        "context_size": GEMMA_CONTEXT_SIZE,
        "coordinator_url": GPU_COORDINATOR_URL,
    }
    
    # Get VRAM usage
    try:
        import subprocess
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=memory.used,memory.total,utilization.gpu", 
             "--format=csv,noheader,nounits"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            info = result.stdout.strip().split('\n')[0].split(',')
            stats["vram_used_mb"] = int(info[0].strip())
            stats["vram_total_mb"] = int(info[1].strip())
            stats["gpu_utilization"] = int(info[2].strip())
    except Exception:
        pass
    
    return stats

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
