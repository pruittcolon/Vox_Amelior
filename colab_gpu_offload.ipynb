{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vox Amelior - GPU Offload Mode\n",
                "\n",
                "This notebook runs ONLY the GPU-heavy components:\n",
                "- **Transcription** (NeMo Parakeet ASR)\n",
                "- **Gemma LLM** (llama.cpp)\n",
                "\n",
                "Your local machine runs everything else. This notebook exposes simple HTTP endpoints via Tailscale.\n",
                "\n",
                "**Setup:**\n",
                "1. Set `TAILSCALE_AUTHKEY` in Colab Secrets\n",
                "2. Run all cells\n",
                "3. Point your local services to the Tailscale IP"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 1. Install Dependencies (GPU)\n",
                "!pip install -q fastapi uvicorn nest_asyncio httpx\n",
                "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "!pip install -q nemo_toolkit[asr]\n",
                "!pip install -q llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
                "!apt-get install -y ffmpeg -qq\n",
                "print(\"Dependencies installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 2. Setup Tailscale\n",
                "import os\n",
                "import subprocess\n",
                "import time\n",
                "from google.colab import userdata\n",
                "\n",
                "!curl -fsSL https://tailscale.com/install.sh | sh\n",
                "\n",
                "AUTHKEY = userdata.get('TAILSCALE_AUTHKEY')\n",
                "if not AUTHKEY:\n",
                "    raise ValueError(\"Set TAILSCALE_AUTHKEY in Colab Secrets!\")\n",
                "\n",
                "subprocess.Popen(\n",
                "    ['sudo', 'tailscaled', '--tun=userspace-networking', '--state=/tmp/tailscale.state'],\n",
                "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
                ")\n",
                "time.sleep(5)\n",
                "\n",
                "result = subprocess.run(\n",
                "    ['sudo', 'tailscale', 'up', f'--authkey={AUTHKEY}', '--hostname=vox-gpu'],\n",
                "    capture_output=True, text=True\n",
                ")\n",
                "if result.returncode != 0:\n",
                "    print(f\"Tailscale error: {result.stderr}\")\n",
                "else:\n",
                "    time.sleep(2)\n",
                "    ip_result = subprocess.run(['tailscale', 'ip', '-4'], capture_output=True, text=True)\n",
                "    TAILSCALE_IP = ip_result.stdout.strip()\n",
                "    print(f\"Tailscale IP: {TAILSCALE_IP}\")\n",
                "    print(f\"Transcription: http://{TAILSCALE_IP}:8003/transcribe\")\n",
                "    print(f\"Gemma:         http://{TAILSCALE_IP}:8001/generate\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 3. Load Transcription Model (NeMo Parakeet)\n",
                "import torch\n",
                "import nemo.collections.asr as nemo_asr\n",
                "\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "\n",
                "# Load Parakeet 1.1B (fast, accurate)\n",
                "print(\"Loading Parakeet ASR model...\")\n",
                "asr_model = nemo_asr.models.ASRModel.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n",
                "asr_model = asr_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "asr_model.eval()\n",
                "print(\"Transcription model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 4. Load Gemma Model (Optional - Comment out if not needed)\n",
                "from llama_cpp import Llama\n",
                "from huggingface_hub import hf_hub_download\n",
                "\n",
                "# Download Gemma 2B 4-bit quantized (small, fast)\n",
                "print(\"Downloading Gemma model...\")\n",
                "model_path = hf_hub_download(\n",
                "    repo_id=\"google/gemma-2b-it-GGUF\",\n",
                "    filename=\"gemma-2b-it.gguf\",\n",
                "    local_dir=\"/content/models\"\n",
                ")\n",
                "\n",
                "print(\"Loading Gemma...\")\n",
                "llm = Llama(\n",
                "    model_path=model_path,\n",
                "    n_ctx=4096,\n",
                "    n_gpu_layers=-1,  # Use all GPU layers\n",
                "    verbose=False\n",
                ")\n",
                "print(\"Gemma loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title 5. Start GPU API Server\n",
                "import io\n",
                "import base64\n",
                "import tempfile\n",
                "import nest_asyncio\n",
                "import uvicorn\n",
                "from fastapi import FastAPI, UploadFile, File\n",
                "from pydantic import BaseModel\n",
                "\n",
                "nest_asyncio.apply()\n",
                "\n",
                "app = FastAPI(title=\"Vox GPU Offload\")\n",
                "\n",
                "class TranscribeRequest(BaseModel):\n",
                "    audio_base64: str  # Base64 encoded audio\n",
                "\n",
                "class GenerateRequest(BaseModel):\n",
                "    prompt: str\n",
                "    max_tokens: int = 512\n",
                "    temperature: float = 0.7\n",
                "\n",
                "@app.get(\"/health\")\n",
                "def health():\n",
                "    return {\"status\": \"healthy\", \"gpu\": torch.cuda.is_available()}\n",
                "\n",
                "@app.post(\"/transcribe\")\n",
                "async def transcribe(request: TranscribeRequest):\n",
                "    \"\"\"Transcribe base64-encoded audio\"\"\"\n",
                "    try:\n",
                "        audio_bytes = base64.b64decode(request.audio_base64)\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
                "            f.write(audio_bytes)\n",
                "            temp_path = f.name\n",
                "        \n",
                "        transcription = asr_model.transcribe([temp_path])\n",
                "        text = transcription[0] if transcription else \"\"\n",
                "        \n",
                "        import os\n",
                "        os.unlink(temp_path)\n",
                "        \n",
                "        return {\"text\": text, \"success\": True}\n",
                "    except Exception as e:\n",
                "        return {\"error\": str(e), \"success\": False}\n",
                "\n",
                "@app.post(\"/transcribe/file\")\n",
                "async def transcribe_file(file: UploadFile = File(...)):\n",
                "    \"\"\"Transcribe uploaded audio file\"\"\"\n",
                "    try:\n",
                "        content = await file.read()\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
                "            f.write(content)\n",
                "            temp_path = f.name\n",
                "        \n",
                "        transcription = asr_model.transcribe([temp_path])\n",
                "        text = transcription[0] if transcription else \"\"\n",
                "        \n",
                "        import os\n",
                "        os.unlink(temp_path)\n",
                "        \n",
                "        return {\"text\": text, \"success\": True}\n",
                "    except Exception as e:\n",
                "        return {\"error\": str(e), \"success\": False}\n",
                "\n",
                "@app.post(\"/generate\")\n",
                "async def generate(request: GenerateRequest):\n",
                "    \"\"\"Generate text with Gemma\"\"\"\n",
                "    try:\n",
                "        output = llm(\n",
                "            request.prompt,\n",
                "            max_tokens=request.max_tokens,\n",
                "            temperature=request.temperature,\n",
                "            stop=[\"<end_of_turn>\", \"<eos>\"]\n",
                "        )\n",
                "        text = output[\"choices\"][0][\"text\"].strip()\n",
                "        return {\"text\": text, \"success\": True}\n",
                "    except Exception as e:\n",
                "        return {\"error\": str(e), \"success\": False}\n",
                "\n",
                "print(\"Starting GPU API server on ports 8001 (Gemma) and 8003 (Transcription)...\")\n",
                "print(f\"Access via Tailscale: http://{TAILSCALE_IP}:8000\")\n",
                "\n",
                "# Run on single port for simplicity\n",
                "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}