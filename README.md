# NeMo AI Ecosystem# Nemo Server



[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://www.python.org/downloads/)

[![Docker](https://img.shields.io/badge/docker-24.0+-blue.svg)](https://www.docker.com/)[![Docker](https://img.shields.io/badge/docker-24.0+-blue.svg)](https://www.docker.com/)

[![CUDA](https://img.shields.io/badge/CUDA-12.6+-green.svg)](https://developer.nvidia.com/cuda-downloads)[![CUDA](https://img.shields.io/badge/CUDA-12.6+-green.svg)](https://developer.nvidia.com/cuda-downloads)

[![Code Lines](https://img.shields.io/badge/code-15K+%20lines-blue)]()[![CI](https://img.shields.io/github/actions/workflow/status/pruittcolon/NeMo_Server/ci.yml?branch=main)](https://github.com/pruittcolon/NeMo_Server/actions)

[![Microservices](https://img.shields.io/badge/architecture-6%20microservices-brightgreen)]()[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)



**Production-Grade Distributed Microservices Platform for Conversational AI****AI-Powered Conversational Memory & Transcription System**



Enterprise-scale voice intelligence system integrating real-time speech recognition, speaker diarization, emotion analysis, semantic memory, and LLM-powered responses. Built for AR wearables, IoT automation, and voice-first applications with intelligent GPU coordination and defense-in-depth security.A microservices-based platform that provides real-time speech transcription, speaker diarization, emotion analysis, semantic memory search, and AI-powered conversational responses. Built for smart glasses and voice-first applications.



------



## ğŸ¯ System Overview## ğŸ¯ What It Does



NeMo Server is a **15,000+ line production codebase** implementing a distributed microservices architecture for conversational AI. The system processes voice input through a coordinated pipeline of specialized services, each optimized for specific AI/ML workloads.Nemo Server transforms conversations into searchable, analyzable knowledge:



### Core Capabilities1. **Transcribe**: Real-time speech-to-text with speaker identification

2. **Analyze**: Emotion detection and audio quality metrics  

```3. **Remember**: Semantic search across all conversations

Voice Input â†’ Transcription â†’ Emotion Analysis â†’ Semantic Memory â†’ LLM Response â†’ Action4. **Respond**: AI assistant with full conversational context

     â†“            â†“                â†“                  â†“              â†“           â†“

  Audio File   Text+Speaker     Sentiment        Context Search   AI Chat    IoT ControlPerfect for:

```- Meeting transcription and analysis

- Smart glasses (AR/VR) voice interfaces

**Key Features:**- Personal memory augmentation

- **Real-time ASR**: NVIDIA NeMo Parakeet (600M params) with sub-second latency- Conversational AI applications

- **Speaker Intelligence**: TitaNet Large diarization + voice enrollment system- Voice-controlled systems

- **Emotion AI**: 6-class sentiment analysis (DistilRoBERTa)

- **Semantic Memory**: FAISS vector search with 384D embeddings---

- **LLM Chat**: Gemma 3 4B with 64K context window

- **GPU Coordination**: Intelligent pause/resume protocol for single-GPU systems## ğŸ—ï¸ Architecture

- **Enterprise Security**: 5-layer defense-in-depth with encrypted databases

- **Multi-Platform**: AR glasses, Flutter mobile, web UI, IoT integration### Microservices Overview



---```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

## ğŸ—ï¸ Architectureâ”‚   Client    â”‚ (Flutter App, Web Browser)

â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜

### Microservices Overview       â”‚

â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

```mermaidâ”‚  API Gateway (Port 8000)                                     â”‚

graph TBâ”‚  â€¢ Authentication & Sessions                                 â”‚

    subgraph "Client Layer"â”‚  â€¢ Request Routing                                           â”‚

        C1[AR Glasses<br/>Even Reality]â”‚  â€¢ Frontend Serving                                          â”‚

        C2[Mobile App<br/>Flutter]â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        C3[Web Browser]       â”‚       â”‚       â”‚       â”‚        â”‚

    end    â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

    â”‚Transâ”‚ â”‚Emo â”‚ â”‚ RAG  â”‚ â”‚Gemma â”‚ â”‚    GPU    â”‚

    subgraph "API Layer"    â”‚criptâ”‚ â”‚tionâ”‚ â”‚Searchâ”‚ â”‚  AI  â”‚ â”‚Coordinatorâ”‚

        GW[API Gateway :8000<br/>Auth, Routing, RBAC]    â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    end       â”‚                       â”‚

       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    subgraph "Service Layer"               â”‚ Shared GPU

        T[Transcription :8003<br/>NeMo ASR + Diarization]         â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”

        G[Gemma AI :8001<br/>LLM Inference]         â”‚  GPU 0    â”‚

        R[RAG :8004<br/>Semantic Search]         â”‚ (NVIDIA)  â”‚

        E[Emotion :8005<br/>Sentiment Analysis]         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        Q[GPU Coordinator :8002<br/>Resource Management]

    endInfrastructure:

  â€¢ Redis (Pub/Sub, Caching, Locking)

    subgraph "Infrastructure Layer"  â€¢ PostgreSQL (Task Queue)

        Redis[(Redis<br/>Pub/Sub + Cache)]  â€¢ Encrypted SQLite (User Data, Transcripts)

        PG[(PostgreSQL<br/>Task Queue)]```

        SQLite[(SQLCipher<br/>Encrypted DBs)]

        FAISS[(FAISS<br/>Vector Index)]### Service Breakdown

    end

| Service | Port | Purpose | GPU | Key Tech |

    subgraph "GPU Layer"|---------|------|---------|-----|----------|

        GPU[NVIDIA GPU<br/>CUDA 12.6+]| **API Gateway** | 8000 | Auth, routing, frontend | No | FastAPI, SQLCipher |

    end| **Transcription** | 8003 | Speech-to-text, diarization | Yes* | NeMo, PyTorch |

| **Emotion** | 8005 | Sentiment analysis | No | Transformers |

    C1 --> GW| **RAG** | 8004 | Semantic search | No | FAISS, Sentence Transformers |

    C2 --> GW| **Gemma AI** | 8001 | LLM chat responses | Yes* | llama.cpp, Gemma 3 |

    C3 --> GW| **GPU Coordinator** | 8002 | GPU sharing | No | Redis, PostgreSQL |

    GW --> T

    GW --> G*GPU is dynamically shared via coordinator

    GW --> R

    GW --> E---

    Q -.Coordinates.-> T

    Q -.Coordinates.-> G## âœ¨ Key Features

    T --> Redis

    G --> Redis### ğŸ™ï¸ Advanced Transcription

    Q --> Redis- **Models**: NVIDIA Parakeet RNNT (600M params)

    Q --> PG- **Speaker Diarization**: Automatic multi-speaker detection

    R --> SQLite- **Speaker Verification**: Match against enrolled voice profiles

    R --> FAISS- **Voice Activity Detection**: Intelligent speech segmentation

    GW --> SQLite- **Real-time Processing**: Sub-second latency per chunk

    T -.GPU Access.-> GPU

    G -.GPU Access.-> GPU### ğŸ˜Š Emotion Analysis

- **6 Emotions**: Joy, sadness, anger, fear, surprise, neutral

    style GW fill:#00aaff,stroke:#0088cc,color:#000- **Confidence Scores**: Per-segment sentiment analysis

    style GPU fill:#76b900,stroke:#5a8f00,color:#000- **Fast**: <100ms per segment

```- **Model**: DistilRoBERTa-base



### Service Responsibilities### ğŸ” Semantic Memory Search

- **Natural Language Queries**: "What did Sarah say about the deadline?"

| Service | Port | Role | GPU | Lines | Key Technologies |- **Vector Search**: FAISS-powered similarity search

|---------|------|------|-----|-------|------------------|- **Rich Filtering**: By speaker, date, emotion

| **API Gateway** | 8000 | Authentication, routing, session management, frontend serving | No | 2,445 | FastAPI, SQLCipher, bcrypt, JWT |- **Cross-Transcript**: Search entire conversation history

| **Transcription** | 8003 | ASR, speaker diarization, voice enrollment, GPU primary owner | Yes* | 2,138 | NeMo, PyTorch 2.4, TitaNet, Parakeet |

| **Gemma AI** | 8001 | LLM inference, RAG-enhanced chat, GPU requester | Yes* | 1,016 | llama.cpp 0.3.16, Gemma 3 4B |### ğŸ¤– AI Assistant (Gemma 3)

| **RAG** | 8004 | Semantic search, memory indexing, context retrieval | No | 3,220 | FAISS, sentence-transformers, SQLCipher |- **64K Context Window**: Long conversation memory

| **Emotion** | 8005 | 6-class sentiment analysis, confidence scoring | No | 448 | Transformers 4.39, DistilRoBERTa |- **RAG-Enhanced**: Automatic context injection from memories

| **GPU Coordinator** | 8002 | Resource scheduling, pause/resume protocol, task queue | No | 1,143 | Redis Pub/Sub, PostgreSQL |- **GPU Shared**: Dynamic GPU coordination with transcription

| **Shared Modules** | - | Auth, crypto, security, storage utilities | No | 4,704 | AES-256, JWT, SQLCipher |- **Streaming**: Token-by-token response streaming



**Total: 15,114 lines of production Python code**### ğŸ” Enterprise Security

- **Encrypted Storage**: SQLCipher for sensitive data

*GPU is dynamically shared via coordinator's pause/resume protocol- **JWT Authentication**: Service-to-service security

- **Replay Protection**: Request ID tracking

---- **Session Management**: Secure cookie-based sessions

- **Docker Secrets**: No credentials in environment vars

## ğŸ” Security Architecture (Defense-in-Depth)

### ğŸš€ GPU Coordination

5-layer security model verified against OWASP Top 10:- **Single GPU Support**: Intelligent sharing between services

- **Pause/Resume**: Sub-second context switching

### Layer 1: Network Security- **No Conflicts**: Redis-based distributed locking

```- **Automatic Fallback**: Graceful degradation on failures

â€¢ CORS with explicit origin whitelisting

â€¢ Rate limiting (120 req/min global, 20 req/min auth)---

â€¢ Request size limits (100MB max)

â€¢ Docker network isolation (services on internal network)## ğŸš€ Quick Start

```

### Prerequisites

### Layer 2: Authentication & Sessions- **GPU**: NVIDIA GPU with 8GB+ VRAM (recommended)

```- **CUDA**: 12.6+ with cuDNN

â€¢ JWT-based session tokens (24h expiry, 1h rotation)- **Docker**: 24.0+ with Docker Compose

â€¢ bcrypt password hashing (cost factor 12)- **RAM**: 16GB+ system memory

â€¢ CSRF double-submit cookie pattern

â€¢ HttpOnly + SameSite=Strict cookies### 1. Clone Repository

â€¢ AES-256-CBC session encryption```bash

```git clone https://github.com/pruittcolon/NeMo_Server.git

cd NeMo_Server

### Layer 3: Service-to-Service Authorization```

```

â€¢ Short-lived JWT tokens (5min TTL) for inter-service auth### 2. Setup Secrets

â€¢ Request ID tracking for replay attack prevention```bash

â€¢ Service identity verification# Generate secure secrets

â€¢ Mutual TLS capable (via Docker secrets)cd docker/secrets

```

# Create random keys

### Layer 4: Data Encryptionopenssl rand -base64 32 > session_key

```openssl rand -base64 32 > jwt_secret

â€¢ SQLCipher AES-256 for databases (users.db, rag.db)openssl rand -base64 32 > users_db_key

â€¢ Encrypted session tokens (32-byte keys)openssl rand -base64 32 > rag_db_key

â€¢ Docker secrets for credential management

â€¢ No plaintext secrets in environment variables# Database credentials

```echo "nemo_user" > postgres_user

openssl rand -base64 16 > postgres_password

### Layer 5: Application Security (RBAC)openssl rand -base64 16 > redis_password

```

â€¢ Role-based access control (Admin, User)# Get Hugging Face token (optional, for model downloads)

â€¢ Speaker-based data isolationecho "hf_your_token_here" > huggingface_token

â€¢ Endpoint-level permission enforcement```

â€¢ Audit logging for sensitive operations

```### 3. Download Models

```bash

**Implementation:**# Gemma 3 model (required for AI chat)

- Authentication: `shared/auth/auth_manager.py` (SessionEncryption class)mkdir -p models

- Authorization: `shared/auth/permissions.py` (require_auth decorator)cd models

- Service Auth: `shared/security/service_jwt.py` (ServiceJWT class)wget https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.gguf

- Database Encryption: `shared/crypto/db_encryption.py`

# NeMo models download automatically on first run

---# Emotion model downloads automatically

```

## âš¡ GPU Coordination Protocol

### 4. Build llama-cpp-python Wheel (for GPU support)

Intelligent GPU sharing enables **single-GPU systems** to run both ASR (Transcription) and LLM (Gemma) without conflicts.```bash

# This must be done on your host machine with CUDA

### ArchitectureCMAKE_ARGS="-DGGML_CUDA=on" pip wheel llama-cpp-python==0.3.16 \

  --wheel-dir=./docker/wheels/ \

```  --no-binary llama-cpp-python

Transcription Service (GPU Owner)    Gemma Service (GPU Requester)

        |                                      |# Or use pre-built wheel if compatible with your CUDA version

        | 1. Parakeet model loaded             | (idle, no GPU)```

        |                                      |

        |                            2. Chat request arrives### 5. Start Services

        |                                      |```bash

        |                            3. Publish request to Redis# Start all services

        |                                      | channel:gemma:request./start.sh

        |                                      |

    4. Receive pause request                  |# Or use docker compose directly

        | channel:transcription:control       |cd docker

        |                                      |docker compose up -d

    5. Pause ASR pipeline (save state)        |```

        | - Stop audio processing              |

        | - Keep models in VRAM                |### 6. Access Web Interface

        |                                      |```bash

    6. ACK pause complete (<100ms)            |# Browser opens automatically, or visit:

        | channel:transcription:status        |open http://localhost:8000

        |                                      |

        |                          7. Acquire GPU lock (Redis)# Default credentials:

        |                                      | SET gpu:lock:current gemma EX 300# Username: admin

        |                                      |# Password: (set during first run or via API)

        |                          8. Load Gemma model (500-800ms)```

        |                                      | llama.cpp â†’ GPU

        |                                      |---

        |                          9. Run inference

        |                                      |## ğŸ“ Project Structure

        |                          10. Release lock, notify complete

        |                                      |```

    11. Resume ASR pipeline                   |Nemo_Server/

        | channel:transcription:control       |â”œâ”€â”€ README.md                 # This file

        |                                      | (unload model)â”œâ”€â”€ start.sh                  # Startup script

```â”œâ”€â”€ .gitignore               # Git ignore rules

â”‚

### Redis Channelsâ”œâ”€â”€ docker/                   # Docker configuration

â”‚   â”œâ”€â”€ docker-compose.yml   # Service orchestration

| Channel | Purpose | Publisher | Subscriber |â”‚   â”œâ”€â”€ Dockerfile.*         # Service-specific builds

|---------|---------|-----------|----------|â”‚   â”œâ”€â”€ secrets/             # Encrypted credentials (gitignored)

| `channel:gemma:request` | Request GPU access | Gemma | GPU Coordinator |â”‚   â””â”€â”€ wheels/              # Pre-built Python wheels

| `channel:transcription:control` | Pause/resume commands | GPU Coordinator | Transcription |â”‚

| `channel:transcription:status` | Acknowledgments | Transcription | GPU Coordinator |â”œâ”€â”€ services/                 # Microservices

| `gpu:lock:current` | Distributed lock | GPU Coordinator | All GPU services |â”‚   â”œâ”€â”€ api-gateway/         # Main entry point

â”‚   â”œâ”€â”€ transcription-service/  # Speech-to-text

### Timing Guaranteesâ”‚   â”œâ”€â”€ emotion-service/     # Sentiment analysis

â”‚   â”œâ”€â”€ rag-service/         # Semantic search

- **Pause ACK**: <100ms (transcription stops processing)â”‚   â”œâ”€â”€ gemma-service/       # AI chat

- **Model Swap**: 500-800ms (unload + load via llama.cpp)â”‚   â””â”€â”€ queue-service/       # GPU coordinator

- **Lock TTL**: 300s (prevents deadlock if service crashes)â”‚

- **Total Overhead**: ~1 second for GPU handoffâ”œâ”€â”€ shared/                   # Shared Python modules

â”‚   â”œâ”€â”€ auth/                # Authentication

**Implementation:** â”‚   â”œâ”€â”€ crypto/              # Encryption utilities

- Coordinator: `services/queue-service/src/main.py`â”‚   â”œâ”€â”€ security/            # Security features

- Transcription pause: `services/transcription-service/src/main.py` (GPUCoordinator class)â”‚   â””â”€â”€ storage/             # Database helpers

- Gemma coordination: `services/gemma-service/src/main.py` (pause_owner/resume_owner)â”‚

â”œâ”€â”€ frontend/                 # Web UI (HTML/JS)

---â”‚   â”œâ”€â”€ index.html

â”‚   â”œâ”€â”€ login.html

## ğŸš€ Quick Startâ”‚   â”œâ”€â”€ transcripts.html

â”‚   â””â”€â”€ assets/

### Prerequisitesâ”‚

â”œâ”€â”€ clients/                  # Client applications

| Requirement | Minimum | Recommended |â”‚   â””â”€â”€ even-demo-app/       # Flutter smart glasses app

|------------|---------|-------------|â”‚

| **GPU** | NVIDIA 8GB VRAM | 12GB+ VRAM (RTX 3060+) |â”œâ”€â”€ models/                   # ML models (gitignored)

| **CUDA** | 12.6+ with cuDNN | Latest stable |â”‚   â””â”€â”€ gemma-3-4b-it-*.gguf

| **RAM** | 16GB | 32GB+ |â”‚

| **Storage** | 25GB free | 50GB+ SSD |â”œâ”€â”€ docker/gateway_instance/  # Gateway runtime data (gitignored)

| **Docker** | 24.0+ | Latest with Docker Compose v2 |â”‚   â”œâ”€â”€ users.db             # User database

â”‚   â”œâ”€â”€ enrollment/          # Speaker audio samples

### Installationâ”‚   â”œâ”€â”€ uploads/             # Uploaded audio/files

â”‚   â””â”€â”€ cache/               # Temporary/cache data

#### 1. Clone Repositoryâ”‚

```bashâ”œâ”€â”€ docker/rag_instance/      # RAG runtime data (gitignored)

git clone https://github.com/pruittcolon/NeMo_Server.gitâ”‚   â””â”€â”€ rag.db               # Memory database (created by service)

cd NeMo_Serverâ”‚

```â”œâ”€â”€ docker/faiss_index/       # Vector index store (gitignored)

â”‚   â”œâ”€â”€ index.bin            # FAISS index

#### 2. Setup Docker Secretsâ”‚   â””â”€â”€ *.docs               # Metadata files

```bashâ”‚

cd docker/secretsâ”œâ”€â”€ logs/                     # Application logs (gitignored)

â”œâ”€â”€ scripts/                  # Utility scripts

# Generate cryptographic keys (Linux/macOS)â””â”€â”€ tests/                    # Test suites

openssl rand -base64 32 > session_key```

openssl rand -base64 32 > jwt_secret

openssl rand -base64 32 > users_db_key---

openssl rand -base64 32 > rag_db_key

## ğŸ“– Documentation

# Database credentials

echo "nemo_user" > postgres_user### Service Documentation

openssl rand -base64 16 > postgres_passwordEach service has detailed documentation:

openssl rand -base64 16 > redis_password- [API Gateway](services/api-gateway/README.md) - Authentication & routing

- [Transcription Service](services/transcription-service/README.md) - Speech-to-text

# Optional: Hugging Face token for model downloads- [Emotion Service](services/emotion-service/README.md) - Sentiment analysis

echo "hf_your_token_here" > huggingface_token- [RAG Service](services/rag-service/README.md) - Semantic search

```- [Gemma Service](services/gemma-service/README.md) - AI chat

- [GPU Coordinator](services/queue-service/README.md) - GPU management

#### 3. Download AI Models

```bash### API Examples

# Gemma 3 LLM (4.5GB)

mkdir -p models#### Transcribe Audio

cd models```bash

wget https://huggingface.co/unsloth/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-UD-Q4_K_XL.ggufcurl -X POST http://localhost:8000/api/transcribe \

  -H "Cookie: session_id=YOUR_SESSION" \

# NeMo and emotion models download automatically on first run  -F "audio=@recording.wav" \

```  -F "enable_diarization=true" \

  -F "enable_emotion=true"

#### 4. Build llama-cpp-python with CUDA Support```

```bash

# Build Python wheel with GPU acceleration#### Semantic Search

CMAKE_ARGS="-DGGML_CUDA=on" pip wheel llama-cpp-python==0.3.16 \```bash

  --wheel-dir=./docker/wheels/ \curl -X POST http://localhost:8000/api/rag/search \

  --no-binary llama-cpp-python  -H "Cookie: session_id=YOUR_SESSION" \

  -H "Content-Type: application/json" \

# Verify wheel created  -d '{

ls docker/wheels/llama_cpp_python-0.3.16-*.whl    "query": "what did they say about the budget?",

```    "top_k": 5,

    "last_n_transcripts": 10

#### 5. Launch Services  }'

```bash```

# Start all 8 containers (6 services + Redis + PostgreSQL)

./start.sh#### Chat with AI

```bash

# Or manually:curl -X POST http://localhost:8000/api/chat \

cd docker  -H "Cookie: session_id=YOUR_SESSION" \

docker compose up -d  -H "Content-Type: application/json" \

  -d '{

# Verify all services healthy    "messages": [

docker compose ps      {"role": "user", "content": "Summarize today's meeting"}

```    ],

    "use_rag": true,

#### 6. Access Web Interface    "max_tokens": 500

```bash  }'

# Open browser (auto-launches)```

open http://localhost:8000

---

# Default admin credentials (change immediately):

# Username: admin## ğŸ”§ Configuration

# Password: admin123

```### Environment Variables



### Verify InstallationKey variables in `docker/.env`:

```bash

# Check all service health endpoints```bash

curl http://localhost:8000/health  # API Gateway# Service URLs (internal)

curl http://localhost:8001/health  # Gemma AIGEMMA_URL=http://gemma-service:8001

curl http://localhost:8002/health  # GPU CoordinatorRAG_URL=http://rag-service:8004

curl http://localhost:8003/health  # TranscriptionEMOTION_URL=http://emotion-service:8005

curl http://localhost:8004/health  # RAGTRANSCRIPTION_URL=http://transcription-service:8003

curl http://localhost:8005/health  # Emotion

# Security

# Check GPU utilizationJWT_ONLY=true

nvidia-smiSESSION_COOKIE_SECURE=false  # Set true for HTTPS

ALLOWED_ORIGINS=http://localhost,http://127.0.0.1

# View logs

docker compose logs -f api-gateway# Transcription

```NEMO_MODEL_NAME=nvidia/parakeet-rnnt-0.6b

ENABLE_PYANNOTE=true

---DIARIZATION_SPK_MAX=3



## ğŸ“Š Technology Stack (Verified)# Gemma AI

GEMMA_MODEL_PATH=/app/models/gemma-3-4b-it-UD-Q4_K_XL.gguf

### Core FrameworkGEMMA_GPU_LAYERS=25

- **Python**: 3.12 (verified across all services)GEMMA_CONTEXT_SIZE=65536

- **FastAPI**: 0.110.3 (async web framework)

- **Uvicorn**: 0.30.6 (ASGI server)# RAG

- **Pydantic**: 2.7-2.10 (data validation)EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

```

### AI/ML Libraries

- **PyTorch**: 2.3.1 - 2.4.1 (deep learning)### Hardware Requirements

- **NVIDIA NeMo**: 2.5+ (speech recognition)

  - Parakeet-CTC-1.1B (ASR)| Component | Minimum | Recommended |

  - TitaNet-Large (speaker embeddings)|-----------|---------|-------------|

- **llama.cpp**: 0.3.16 (LLM inference)| GPU VRAM | 8GB | 12GB+ |

  - Gemma 3 4B Q4_K_XL quantized| System RAM | 16GB | 32GB+ |

- **Transformers**: 4.39.3 (Hugging Face)| Storage | 20GB | 50GB+ |

  - DistilRoBERTa-base (emotion)| CPU | 4 cores | 8+ cores |

- **sentence-transformers**: 2.7.0 (embeddings)

  - all-MiniLM-L6-v2 (384D vectors)---

- **FAISS**: 1.8.0 (vector search)

## ğŸ§ª Testing

### Infrastructure

- **Redis**: 5.0.1 (Pub/Sub, caching, locks)```bash

- **PostgreSQL**: 14+ (task queue)# Run all tests (unit + smoke + security by default)

- **SQLCipher**: 1.0.4 (encrypted SQLite)./scripts/run_tests.sh

- **Docker**: 24.0+ with Compose v2

# Unit tests only

### Security & Authpytest -m unit -v

- **bcrypt**: 4.1.2 (password hashing)

- **python-jose**: 3.3.0 (JWT)# Integration tests (requires services running; opt-in)

- **cryptography**: 42.0.5 (AES-256)RUN_INTEGRATION=1 pytest -m integration -v



### Audio Processing# Smoke tests (gateway health)

- **librosa**: 0.10.2 (audio analysis)pytest -m smoke -v

- **soundfile**: 0.12.1 (I/O)```

- **pyannote.audio**: 3.1.1 (diarization)

---

---

## ğŸ› ï¸ Development

## ğŸ“ Project Structure

### Running Services Individually

```

NeMo_Server/```bash

â”œâ”€â”€ services/                      # 6 microservices (15K lines)# API Gateway

â”‚   â”œâ”€â”€ api-gateway/              # Port 8000 - Entry point (2.4K lines)cd services/api-gateway

â”‚   â”‚   â”œâ”€â”€ src/main.py           # FastAPI app, routing, authuvicorn src.main:app --reload --port 8000

â”‚   â”‚   â””â”€â”€ requirements.txt      # Dependencies

â”‚   â”œâ”€â”€ transcription-service/    # Port 8003 - ASR (2.1K lines)# Transcription Service

â”‚   â”‚   â”œâ”€â”€ src/main.py           # NeMo pipeline, GPU coordinationcd services/transcription-service

â”‚   â”‚   â””â”€â”€ requirements.txt      # PyTorch, NeMo, pyannoteuvicorn src.main:app --reload --port 8003

â”‚   â”œâ”€â”€ gemma-service/            # Port 8001 - LLM (1.0K lines)

â”‚   â”‚   â”œâ”€â”€ src/main.py           # llama.cpp inference, RAG# etc.

â”‚   â”‚   â””â”€â”€ requirements.txt      # llama-cpp-python```

â”‚   â”œâ”€â”€ rag-service/              # Port 8004 - Search (3.2K lines)

â”‚   â”‚   â”œâ”€â”€ src/main.py           # FAISS indexing, SQLCipher### Debugging

â”‚   â”‚   â””â”€â”€ requirements.txt      # sentence-transformers, FAISS

â”‚   â”œâ”€â”€ emotion-service/          # Port 8005 - Sentiment (448 lines)```bash

â”‚   â”‚   â”œâ”€â”€ src/main.py           # DistilRoBERTa pipeline# View logs

â”‚   â”‚   â””â”€â”€ requirements.txt      # transformers, torchdocker compose logs -f api-gateway

â”‚   â””â”€â”€ queue-service/            # Port 8002 - GPU coordinator (1.1K lines)

â”‚       â”œâ”€â”€ src/main.py           # Redis Pub/Sub, PostgreSQL queue# Check GPU usage

â”‚       â””â”€â”€ requirements.txt      # redis, asyncpgnvidia-smi -l 1

â”‚

â”œâ”€â”€ shared/                        # Shared utilities (4.7K lines)# Redis CLI

â”‚   â”œâ”€â”€ auth/                     # Authentication & RBACdocker exec -it refactored_redis redis-cli

â”‚   â”‚   â”œâ”€â”€ auth_manager.py       # User management, sessions

â”‚   â”‚   â””â”€â”€ permissions.py        # Role-based access control# PostgreSQL CLI

â”‚   â”œâ”€â”€ crypto/                   # Encryption utilitiesdocker exec -it refactored_postgres psql -U nemo_user nemo_queue

â”‚   â”‚   â””â”€â”€ db_encryption.py      # SQLCipher wrapper```

â”‚   â”œâ”€â”€ security/                 # Security features

â”‚   â”‚   â”œâ”€â”€ service_jwt.py        # Inter-service auth---

â”‚   â”‚   â””â”€â”€ secrets.py            # Docker secrets loader

â”‚   â””â”€â”€ storage/                  # Database helpers## ğŸ“Š Monitoring

â”‚

â”œâ”€â”€ docker/                        # Container orchestration### Health Checks

â”‚   â”œâ”€â”€ docker-compose.yml        # 8 services definition```bash

â”‚   â”œâ”€â”€ Dockerfile.api            # API Gateway image# All services

â”‚   â”œâ”€â”€ Dockerfile.transcription  # Transcription image (CUDA)curl http://localhost:8000/health

â”‚   â”œâ”€â”€ Dockerfile.gemma          # Gemma image (CUDA)curl http://localhost:8001/health

â”‚   â”œâ”€â”€ Dockerfile.rag            # RAG imagecurl http://localhost:8003/health

â”‚   â”œâ”€â”€ Dockerfile.emotion        # Emotion imagecurl http://localhost:8004/health

â”‚   â”œâ”€â”€ Dockerfile.queue          # GPU Coordinator imagecurl http://localhost:8005/health

â”‚   â”œâ”€â”€ secrets/                  # Encrypted credentials (gitignored)curl http://localhost:8002/health

â”‚   â”‚   â”œâ”€â”€ session_key           # 32-byte AES key```

â”‚   â”‚   â”œâ”€â”€ jwt_secret            # JWT signing key

â”‚   â”‚   â”œâ”€â”€ users_db_key          # SQLCipher key### Metrics

â”‚   â”‚   â”œâ”€â”€ rag_db_key            # SQLCipher key- GPU utilization: `nvidia-smi`

â”‚   â”‚   â”œâ”€â”€ postgres_password     # DB password- Service logs: `docker compose logs`

â”‚   â”‚   â””â”€â”€ redis_password        # Redis password- Redis: `redis-cli INFO`

â”‚   â””â”€â”€ wheels/                   # Pre-built Python wheels- PostgreSQL: `psql` queries

â”‚       â””â”€â”€ llama_cpp_python-0.3.16-*-linux_x86_64.whl

â”‚---

â”œâ”€â”€ frontend/                      # Web UI (HTML/CSS/JS)

â”‚   â”œâ”€â”€ login.html                # Authentication## ğŸ¤ Contributing

â”‚   â”œâ”€â”€ transcripts.html          # Conversation history

â”‚   â”œâ”€â”€ search.html               # Semantic search1. Fork the repository

â”‚   â”œâ”€â”€ emotions.html             # Sentiment dashboard2. Create a feature branch

â”‚   â””â”€â”€ gemma.html                # AI chat interface3. Make your changes

â”‚4. Add tests

â”œâ”€â”€ models/                        # AI models (gitignored)5. Submit a pull request

â”‚   â”œâ”€â”€ gemma-3-4b-it-UD-Q4_K_XL.gguf  # 4.5GB quantized LLM

â”‚   â”œâ”€â”€ emotion-english-distilroberta-base/  # 255MB sentimentSee `.github/PULL_REQUEST_TEMPLATE.md` for PR guidelines.

â”‚   â””â”€â”€ (NeMo models auto-downloaded to ~/.cache/torch/NeMo/)

â”‚---

â”œâ”€â”€ docker/gateway_instance/       # Runtime data (gitignored)

â”‚   â”œâ”€â”€ users.db                  # SQLCipher encrypted user DB## ğŸ“„ License

â”‚   â”œâ”€â”€ enrollment/               # Speaker voice profiles

â”‚   â””â”€â”€ uploads/                  # Audio filesThis project includes third-party components:

â”‚- **NeMo**: Apache 2.0 License

â”œâ”€â”€ docker/rag_instance/           # Runtime data (gitignored)- **Gemma Models**: Gemma Terms of Use

â”‚   â””â”€â”€ rag.db                    # SQLCipher encrypted memory DB- **Transformers**: Apache 2.0 License

â”‚- **FAISS**: MIT License

â”œâ”€â”€ docker/faiss_index/            # Vector index (gitignored)

â”‚   â”œâ”€â”€ index.bin                 # FAISS index file---

â”‚   â””â”€â”€ *.docs                    # Metadata

â”‚## ğŸ™ Acknowledgments

â”œâ”€â”€ scripts/                       # Utility scripts

â”‚   â”œâ”€â”€ run_tests.sh              # Test runner- **NVIDIA NeMo**: State-of-the-art ASR models

â”‚   â”œâ”€â”€ healthcheck.sh            # Service health check- **Google**: Gemma 3 language model

â”‚   â””â”€â”€ security_hardening.py     # Security audit- **Hugging Face**: Model hosting and transformers

â”‚- **llama.cpp**: Efficient LLM inference

â”œâ”€â”€ tests/                         # Test suites- **Even Realities**: Smart glasses platform inspiration

â”‚   â”œâ”€â”€ unit/                     # Unit tests

â”‚   â”œâ”€â”€ integration/              # Integration tests---

â”‚   â”œâ”€â”€ security/                 # Security tests

â”‚   â””â”€â”€ conftest.py               # Pytest configuration## ğŸ“ Support

â”‚

â”œâ”€â”€ ARCHITECTURE.md                # Detailed architecture docs- **Issues**: [GitHub Issues](https://github.com/pruittcolon/NeMo_Server/issues)

â”œâ”€â”€ README.md                      # This file- **Discussions**: [GitHub Discussions](https://github.com/pruittcolon/NeMo_Server/discussions)

â”œâ”€â”€ start.sh                       # Startup script

â””â”€â”€ docker-compose.yml             # Legacy location (points to docker/)---

```

**Built with â¤ï¸ for the future of conversational AI**

---

## ğŸ”Œ API Reference

### Authentication
```bash
# Login
curl -X POST http://localhost:8000/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin123"}'

# Response includes session cookie
```

### Transcription
```bash
# Transcribe audio with diarization and emotion
curl -X POST http://localhost:8000/api/transcription/transcribe \
  -H "Cookie: ws_session=YOUR_SESSION_TOKEN" \
  -F "audio=@recording.wav" \
  -F "enable_diarization=true" \
  -F "enable_emotion=true"

# Response:
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "text": "Hello, how are you?",
  "segments": [
    {
      "text": "Hello, how are you?",
      "speaker": "SPEAKER_00",
      "start": 0.0,
      "end": 2.5,
      "emotion": "neutral",
      "confidence": 0.89
    }
  ],
  "processing_time": 0.34
}
```

### Semantic Search
```bash
# Search memories by natural language
curl -X POST http://localhost:8000/api/memory/search \
  -H "Cookie: ws_session=YOUR_SESSION_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "what did Sarah say about the deadline?",
    "top_k": 5,
    "filters": {
      "speaker": "SPEAKER_00",
      "emotion": "neutral"
    }
  }'

# Response:
{
  "results": [
    {
      "text": "Sarah mentioned the deadline is next Friday",
      "score": 0.87,
      "speaker": "SPEAKER_00",
      "timestamp": "2024-11-03T14:23:10Z",
      "emotion": "neutral"
    }
  ],
  "query_time_ms": 12
}
```

### AI Chat (RAG-Enhanced)
```bash
# Chat with context from memories
curl -X POST http://localhost:8000/api/gemma/chat \
  -H "Cookie: ws_session=YOUR_SESSION_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Summarize today'\''s meeting"}
    ],
    "use_rag": true,
    "max_tokens": 500,
    "temperature": 0.7
  }'

# Response (streaming):
{
  "response": "Based on your conversation history, today's meeting covered...",
  "context_used": ["segment_id_1", "segment_id_2"],
  "tokens_generated": 127,
  "generation_time": 2.3
}
```

### Emotion Analysis
```bash
# Analyze sentiment of text
curl -X POST http://localhost:8000/api/emotion/analyze \
  -H "Cookie: ws_session=YOUR_SESSION_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"text": "I am extremely happy about this news!"}'

# Response:
{
  "emotion": "joy",
  "confidence": 0.94,
  "all_scores": {
    "joy": 0.94,
    "neutral": 0.03,
    "surprise": 0.02,
    "sadness": 0.01,
    "anger": 0.00,
    "fear": 0.00
  }
}
```

---

## ğŸ§ª Testing

### Test Coverage

```bash
# Run all tests
./scripts/run_tests.sh

# Unit tests only (fast)
pytest -m unit -v

# Integration tests (requires running services)
RUN_INTEGRATION=1 pytest -m integration -v

# Security tests
pytest -m security -v

# Smoke tests (health checks)
pytest -m smoke -v
```

### Test Suites

| Suite | Files | Purpose | Duration |
|-------|-------|---------|----------|
| **Unit** | 25+ | Service logic, utilities | <30s |
| **Integration** | 10+ | End-to-end API flows | 2-5min |
| **Security** | 8+ | Auth, encryption, RBAC | <1min |
| **Smoke** | 5+ | Health checks, connectivity | <10s |

---

## ğŸ”§ Configuration

### Environment Variables

Key settings in `docker/.env` or service-specific configs:

```bash
# === SERVICE URLS (Internal Docker Network) ===
GEMMA_URL=http://gemma-service:8001
RAG_URL=http://rag-service:8004
EMOTION_URL=http://emotion-service:8005
TRANSCRIPTION_URL=http://transcription-service:8003

# === SECURITY ===
JWT_ONLY=true                          # Enforce JWT for inter-service
SESSION_COOKIE_SECURE=false            # Set true for HTTPS
SESSION_COOKIE_SAMESITE=strict         # CSRF protection
ALLOWED_ORIGINS=http://localhost,http://127.0.0.1
RATE_LIMIT_DEFAULT=120                 # Requests per minute
RATE_LIMIT_AUTH=20                     # Auth requests per minute

# === TRANSCRIPTION SERVICE ===
NEMO_MODEL_NAME=nvidia/parakeet-rnnt-0.6b  # ASR model
ENABLE_PYANNOTE=true                   # Speaker diarization
DIARIZATION_SPK_MAX=3                  # Max speakers to detect
MIN_SPEECH_DURATION=0.3                # VAD threshold (seconds)

# === GEMMA SERVICE ===
GEMMA_MODEL_PATH=/app/models/gemma-3-4b-it-UD-Q4_K_XL.gguf
GEMMA_GPU_LAYERS=25                    # Layers offloaded to GPU
GEMMA_CONTEXT_SIZE=65536               # 64K context window
GEMMA_TEMPERATURE=0.7                  # Response creativity

# === RAG SERVICE ===
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
FAISS_INDEX_TYPE=IndexFlatIP           # Cosine similarity
RAG_TOP_K=5                            # Results per query

# === GPU COORDINATION ===
GPU_LOCK_TTL=300                       # Lock timeout (seconds)
PAUSE_TIMEOUT=5                        # Max pause wait (seconds)
```

---

## ğŸš¨ Troubleshooting

### Common Issues

#### Services Won't Start
```bash
# Check Docker resources
docker system df
docker system prune -a  # Free space

# Check CUDA availability
nvidia-smi

# Verify secrets exist
ls -la docker/secrets/
```

#### GPU Out of Memory
```bash
# Reduce Gemma GPU layers in docker/.env
GEMMA_GPU_LAYERS=15  # Default: 25

# Or use CPU-only mode
GEMMA_GPU_LAYERS=0
```

#### Slow Transcription
```bash
# Check GPU utilization
nvidia-smi -l 1

# Verify NeMo model loaded
docker compose logs transcription-service | grep "Model loaded"

# Check if GPU coordinator is functioning
curl http://localhost:8002/health
```

#### Authentication Failures
```bash
# Verify secrets are valid base64
cat docker/secrets/session_key | base64 -d | wc -c  # Should be 32

# Reset admin password (if locked out)
docker compose exec api-gateway python -c "
from shared.auth.auth_manager import AuthManager
am = AuthManager(db_path='/app/instance/users.db')
user = am.get_user('admin')
user.password_hash = am._hash_password('newpassword')
am._save_user(user)
"
```

---

## ğŸ› ï¸ Development

### Running Services Individually

```bash
# API Gateway
cd services/api-gateway
pip install -r requirements.txt
python -m uvicorn src.main:app --reload --port 8000

# Transcription (requires CUDA)
cd services/transcription-service
pip install -r requirements.txt
python -m uvicorn src.main:app --reload --port 8003
```

### Debugging

```bash
# Live logs with color
docker compose logs -f --tail=100 api-gateway

# Access container shell
docker compose exec api-gateway /bin/bash

# Check Redis Pub/Sub
docker compose exec redis redis-cli
> SUBSCRIBE channel:transcription:control
> PUBLISH channel:gemma:request "test"

# Query PostgreSQL
docker compose exec postgres psql -U nemo_user nemo_queue
> SELECT * FROM gpu_tasks ORDER BY created_at DESC LIMIT 10;

# GPU memory usage
nvidia-smi --query-gpu=memory.used,memory.free --format=csv -l 1
```

---

## ğŸ¤ Contributing

Contributions welcome! This project follows production-grade standards:

1. **Fork & Branch**: Create feature branch from `v2-modular`
2. **Code Style**: Black formatting, type hints, docstrings
3. **Testing**: Add unit tests (>80% coverage required)
4. **Security**: No secrets in commits, follow OWASP guidelines
5. **Documentation**: Update README and ARCHITECTURE.md
6. **Pull Request**: Use PR template, link related issues

```bash
# Format code
black services/ shared/

# Run linters
flake8 services/ shared/
mypy services/ shared/

# Run tests
pytest -v --cov=services --cov=shared
```

---

## ğŸ“„ License & Acknowledgments

### License
MIT License - See [LICENSE](LICENSE) for details

### Third-Party Components

| Component | License | Purpose |
|-----------|---------|---------|
| **NVIDIA NeMo** | Apache 2.0 | Speech recognition models |
| **Google Gemma** | Gemma Terms | Language model |
| **PyTorch** | BSD-3-Clause | Deep learning framework |
| **llama.cpp** | MIT | LLM inference engine |
| **FAISS** | MIT | Vector similarity search |
| **FastAPI** | MIT | Web framework |

### Acknowledgments

- **NVIDIA**: NeMo Toolkit and pre-trained ASR models
- **Google DeepMind**: Gemma 3 language model
- **Hugging Face**: Model hosting and Transformers library
- **Georgi Gerganov**: llama.cpp inference engine
- **Meta Research**: FAISS vector search

---

## ğŸ“ Support & Contact

- **GitHub Issues**: [Report bugs](https://github.com/pruittcolon/NeMo_Server/issues)
- **Discussions**: [Ask questions](https://github.com/pruittcolon/NeMo_Server/discussions)
- **Portfolio**: [whyhirepruitt.dev](https://whyhirepruitt.dev)

---

**Built with â¤ï¸ for production AI systems**

*NeMo Server v2.0 - Enterprise-grade conversational AI platform*
